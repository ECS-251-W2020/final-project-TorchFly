{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import json\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"/data/SECTOR/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_sectors = len(os.listdir(corpus_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnCorpusDataset:\n",
    "    def __init__(self, corpus_path:str, rank:int, total_num_sectors:int):\n",
    "        self.corpus_path = corpus_path\n",
    "        self.rank = rank\n",
    "        self.total_num_sectors = total_num_sectors\n",
    "        \n",
    "        # current sector pointer\n",
    "        self.cur_sector_id = rank\n",
    "        \n",
    "        # extra\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_sector(corpus_path, sector_id:int):\n",
    "        with open(os.path.join(corpus_path, str(sector_id) + \".jsonl\"), \"r\") as f:\n",
    "            data = f.readlines()\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_data(data:List[str]):\n",
    "        example = json.loads(line)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(corpus_path, str(0) + \".jsonl\"), \"r\") as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_splited_delimiters(sents, delimiter):\n",
    "    return sents\n",
    "\n",
    "count = 0\n",
    "\n",
    "def process_sentences(sentences:List[str], state:int=0) -> List[np.array]:\n",
    "    global count\n",
    "    delimiters = [\",\", \"。\", \"、\", \"，\", \"\\n\", \":\", \";\", '.']\n",
    "    \n",
    "    token_sents = []\n",
    "    for sent in sentences:\n",
    "        token_sent = tokenizer.tokenize(sent)\n",
    "        \n",
    "        if len(token_sent) > max_seq_length:\n",
    "            # recursive state machine\n",
    "            if state == 0:\n",
    "                sents = sent.split(\"\\n\")\n",
    "                sents = [sent + \"\\n\" for sent in sents[:-1]] + [sents[-1]]\n",
    "                sub_token_sents = process_sentences(sents, state=1)\n",
    "                if sub_token_sents:\n",
    "                    token_sents.extend(sub_token_sents)\n",
    "            elif state == 1:\n",
    "                sents = sent.split(\".\")  \n",
    "                sents = [sent + \".\" for sent in sents[:-1]] + [sents[-1]]\n",
    "                sub_token_sents = process_sentences(sents, state=2)\n",
    "                if sub_token_sents:\n",
    "                    token_sents.extend(sub_token_sents)\n",
    "            elif state == 2:\n",
    "                sents = sent.split(\",\")  \n",
    "                sents = [sent + \",\" for sent in sents[:-1]] + [sents[-1]]\n",
    "                sub_token_sents = process_sentences(sents, state=3)\n",
    "                if sub_token_sents:\n",
    "                    token_sents.extend(sub_token_sents)\n",
    "            else:\n",
    "#                 print(state)\n",
    "                #print(f\"{len(token_sent)} for sub sentences\")\n",
    "#                 print(sent)\n",
    "                count += 1\n",
    "                return None\n",
    "                # breakpoint()\n",
    "            \n",
    "        else:\n",
    "            token_sents.append(token_sent)\n",
    "            \n",
    "    return token_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(sentences:List[str], state:int=0) -> List[np.array]:\n",
    "    \n",
    "    token_sents = []\n",
    "    for sent in sentences:\n",
    "        token_sent = tokenizer.tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b16b89d4e6447fb48ecfc65cec7484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=215525), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for line in tqdm.tqdm_notebook(data):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = json.loads(line)[\"sents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850b48399f464fb8889fbf0deefd4a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=215525), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_data = []\n",
    "flag = False\n",
    "\n",
    "for line in tqdm.tqdm_notebook(data):\n",
    "    example = json.loads(line)\n",
    "    token_sents = process(example[\"sents\"])\n",
    "    \n",
    "    new_data.append(token_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusDataLoader:\n",
    "    def __init__(self, corpus_path):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_data[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215525"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def __init__(self, corpus_path:str, rank:int, total_num_sectors:int):\n",
    "    self.corpus_path = corpus_path\n",
    "    self.rank = rank\n",
    "    self.total_num_sectors = total_num_sectors\n",
    "\n",
    "    # current sector pointer\n",
    "    self.cur_sector_id = rank\n",
    "\n",
    "    # extra\n",
    "    self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusIterator:\n",
    "    def __init__(self, processed_docs):\n",
    "        self.processed_docs = processed_docs\n",
    "        self.total_num_docs = len(processed_docs)\n",
    "\n",
    "    def __iter__ (self):\n",
    "        # shuffle the indices\n",
    "        indices = np.arange(self.total_num_docs)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for doc_index in indices:\n",
    "            # randomly sample a document\n",
    "            doc = self.processed_docs[doc_index]\n",
    "\n",
    "            for i, segment in enumerate(doc):\n",
    "                # output if the segment is the start of the document\n",
    "                yield segment, i==0\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sents_to_segments(tokenizer, doc_sentences):\n",
    "\n",
    "    token_segments = []\n",
    "    current_seq = []\n",
    "\n",
    "    for count, sent in enumerate(doc_sentences):\n",
    "        if count > 0:\n",
    "            sent = \" \" + sent\n",
    "\n",
    "        token_sent = tokenizer.tokenize(sent)\n",
    "\n",
    "        if len(token_sent) > max_seq_length:\n",
    "            # append last sequence\n",
    "            token_segments.append(current_seq)\n",
    "\n",
    "            for i in range(0, len(token_sent) - max_seq_length, max_seq_length):\n",
    "                token_segments.append(token_sent[i:i+max_seq_length])\n",
    "\n",
    "            # assign the current seq the tail of token_sent\n",
    "            current_seq = token_sent[i+max_seq_length:i+max_seq_length*2]\n",
    "            continue\n",
    "\n",
    "        if (len(current_seq) + len(token_sent)) > max_seq_length:\n",
    "            token_segments.append(current_seq)\n",
    "            current_seq = token_sent\n",
    "        else:\n",
    "            current_seq = current_seq + token_sent\n",
    "    \n",
    "    if len(current_seq) > 0:\n",
    "        token_segments.append(current_seq)\n",
    "    \n",
    "    # remove empty segment\n",
    "    token_segments = [seg for seg in token_segments if seg]\n",
    "    \n",
    "    return token_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(tokenizer, corpus_path:str, sector_id:int, cache_dir:str = None):\n",
    "    if cache_dir:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        cache_path = os.path.join(cache_dir, f\"{sector_id}_cache.pkl\")\n",
    "        \n",
    "        if os.path.exists(cache_path):\n",
    "            try:\n",
    "                processed_docs = torch.load(cache_path)\n",
    "                return processed_docs\n",
    "            except:\n",
    "                print(\"File Corrupted. Data will be re-processed\")\n",
    "        \n",
    "    # processing data\n",
    "    with open(os.path.join(corpus_path, str(sector_id) + \".jsonl\"), \"r\") as f:\n",
    "        data = f.readlines()\n",
    "    \n",
    "    processed_docs = []\n",
    "\n",
    "    print(\"Processing Data. Takes about 10 mins\")\n",
    "    \n",
    "    for line in data:\n",
    "        example = json.loads(line)\n",
    "        token_segments = sents_to_segments(tokenizer, example[\"sents\"])\n",
    "\n",
    "        processed_docs.append(token_segments)    \n",
    "    \n",
    "    if cache_dir:\n",
    "        print(\"Saving Into Cache\")\n",
    "        torch.save(processed_docs, f\"{sector_id}_cache.pkl\")\n",
    "        \n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusBatchIterator:\n",
    "    def __init__(self, corpus_path:str, tokenizer, batch_size:int, rank:int):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        corpus_path: directory path to store the corpus sectors\n",
    "        rank: for distributed learning.\n",
    "    \"\"\" \n",
    "        self.tokenizer = tokenize\n",
    "        self.batch_size = batch_size\n",
    "        self.current_sector_id = 0\n",
    "        self.total_num_sectors = len(os.listdir(corpus_path))\n",
    "        \n",
    "        # process the data and save it into cache\n",
    "        \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            self.iterators = [CorpusIterator(processed_docs) for i in range(self.batch_size)]\n",
    "            \n",
    "            try:\n",
    "                # TODO: extend it with Ray\n",
    "                batch = [next(self.iterators[i]) for i in range(self.batch_size)]\n",
    "                yield batch\n",
    "            except StopIteration:\n",
    "                # after the iterator finishes, load the next sector\n",
    "                processed_docs = self.load_corpus()\n",
    "                self.iterators = [CorpusIterator(processed_docs) for i in range(self.batch_size)]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_gen = iter(CorpusGenerator(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['This',\n",
       "  'Ġis',\n",
       "  'Ġparticularly',\n",
       "  'Ġimportant',\n",
       "  'Ġwhen',\n",
       "  'Ġconsidering',\n",
       "  'Ġthe',\n",
       "  'Ġtypes',\n",
       "  'Ġof',\n",
       "  'Ġfirearms',\n",
       "  'Ġtypically',\n",
       "  'Ġinvolved',\n",
       "  'Ġin',\n",
       "  'Ġcrime',\n",
       "  '.',\n",
       "  '\"',\n",
       "  'Policy',\n",
       "  'Ġmakers',\n",
       "  'Ġaren',\n",
       "  \"'t\",\n",
       "  'Ġlooking',\n",
       "  'Ġat',\n",
       "  'Ġcurrent',\n",
       "  'Ġdata',\n",
       "  ',\"',\n",
       "  'Ġstated',\n",
       "  'ĠJeff',\n",
       "  'ĠMonroe',\n",
       "  ',',\n",
       "  'ĠPh',\n",
       "  '.',\n",
       "  'D',\n",
       "  '.',\n",
       "  'Ġand',\n",
       "  'Ġpresident',\n",
       "  'Ġof',\n",
       "  'ĠThink',\n",
       "  '22',\n",
       "  'three',\n",
       "  '.',\n",
       "  'When',\n",
       "  'Ġasked',\n",
       "  'Ġabout',\n",
       "  'Ġtheir',\n",
       "  'Ġmost',\n",
       "  'Ġrecent',\n",
       "  'Ġtwo',\n",
       "  'Ġfirearms',\n",
       "  'Ġpurchases',\n",
       "  ',',\n",
       "  'Ġ89',\n",
       "  '.',\n",
       "  '7',\n",
       "  'Ġpercent',\n",
       "  'Ġof',\n",
       "  'Ġgun',\n",
       "  '-',\n",
       "  'own',\n",
       "  'ing',\n",
       "  'Ġsurvey',\n",
       "  'Ġrespondents',\n",
       "  'Ġstated',\n",
       "  'Ġthat',\n",
       "  'Ġthey',\n",
       "  'Ġpurchased',\n",
       "  'Ġtheir',\n",
       "  'Ġfirearms',\n",
       "  'Ġfrom',\n",
       "  'Ġa',\n",
       "  'Ġdealer',\n",
       "  'Ġthat',\n",
       "  'Ġconducted',\n",
       "  'Ġa',\n",
       "  'Ġbackground',\n",
       "  'Ġcheck',\n",
       "  '.',\n",
       "  'Ġ\"',\n",
       "  'Sep',\n",
       "  'ar',\n",
       "  'ating',\n",
       "  'Ġthe',\n",
       "  'Ġdata',\n",
       "  'Ġby',\n",
       "  'Ġthe',\n",
       "  'Ġtype',\n",
       "  'Ġof',\n",
       "  'Ġfirearm',\n",
       "  'Ġpurchased',\n",
       "  'Ġclar',\n",
       "  'ifies',\n",
       "  'Ġfirearms',\n",
       "  'Ġpurchases',\n",
       "  'Ġfurther',\n",
       "  '.',\n",
       "  'Shot',\n",
       "  'guns',\n",
       "  ',',\n",
       "  'Ġfor',\n",
       "  'Ġexample',\n",
       "  ',',\n",
       "  'Ġare',\n",
       "  'Ġfar',\n",
       "  'Ġless',\n",
       "  'Ġlikely',\n",
       "  'Ġthan',\n",
       "  'Ġrevol',\n",
       "  'vers',\n",
       "  ',',\n",
       "  'Ġpistols',\n",
       "  'Ġand',\n",
       "  'Ġrifles',\n",
       "  'Ġto',\n",
       "  'Ġbe',\n",
       "  'Ġpurchased',\n",
       "  'Ġfrom',\n",
       "  'Ġa',\n",
       "  'Ġdealer',\n",
       "  'Ġconducting',\n",
       "  'Ġa',\n",
       "  'Ġbackground',\n",
       "  'Ġcheck',\n",
       "  ',\"',\n",
       "  'Ġstates',\n",
       "  'ĠMonroe',\n",
       "  '.'],\n",
       " False)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(corpus_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CorpusGenerator at 0x7f210f07c5d0>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a_number():\n",
    "    count = 0\n",
    "    while True:\n",
    "        count = count + 1\n",
    "        yield count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_a_number()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sent = [1,2,3,4,5,6,7,8,9,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 6]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0, len(token_sent)-3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
