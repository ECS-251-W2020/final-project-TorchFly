exbert:
  attention_probs_dropout_prob: 0.0
  embedding_std: 0.01
  gradient_checkpointing: false
  hidden_act: h_swish
  hidden_dropout_prob: 0.0
  hidden_size: 768
  intermediate_size: 3072
  layer_norm_eps: 1.0e-05
  max_position_embeddings: 512
  num_attention_heads: 12
  num_hidden_layers: 12
  pad_token_id: 1
  persistent_mem_size: 64
  vocab_size: 50265
training:
  fp16: true
  fp16_opt_level: O1
  gradient_accumulation_steps: 1
  learning_rate: 1.0e-05
  max_grad_norm: 1.0
  num_gpus: 1
  optimizer: AdamW
