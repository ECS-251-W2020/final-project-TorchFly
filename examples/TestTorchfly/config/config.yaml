training:
    num_gpus: 1 
    fp16: true
    fp16_opt_level: "O1" # O0 to disable fp16
    # optimization
    learning_rate: 1e-5
    gradient_accumulation_steps: 1
    max_grad_norm: 1.0 # disabled when negative
    optimizer: "AdamW"
    batch_size: 32
    total_num_epochs: 10
    # logging
    log_iterations_interval: -1 # disabled when negative
    log_seconds_interval: 1 # disabled when `log_iterations_interval` is set